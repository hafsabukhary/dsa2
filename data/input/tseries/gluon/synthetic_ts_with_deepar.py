# -*- coding: utf-8 -*-
"""Synthetic ts with deepar.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FxVsGuf9fQ8V_49jH32e_-FyBjARCvKj
"""



#### Install MLMODELS in Colab

#!  bash <(wget -qO- https://cutt.ly/mlmodels)
#!pip install pydantic==1.4 --force

import os, time
os.kill(os.getpid(), 9)


#Working directory
import os
dirdrive = "/content/drive/My Drive/Colab Notebooks/shared_session/timeseries_example"
os.chdir(dirdrive)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import mxnet as mx
from mxnet import gluon
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import os
from tqdm.autonotebook import tqdm
from pathlib import Path


# https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/
from pandas import DataFrame
from pandas import Series
from pandas import concat
from pandas import read_csv
from pandas import datetime

from matplotlib import pyplot
import numpy as np

timesteps=1
interval=1
test_values=30
batch_size, nb_epoch, neurons  = 1,10,1
#create data
from util_gluonts import create_timeseries1d



######################################

train_df=create_timeseries1d(expression = '(20*np.sin(2*np.pi *2* index/100))',t= True,start_date = '1/1/2000')

pyplot.plot(train_df['value'][0:80])

train_df.head(2)



# load dataset
data_folder = "kaggle_data/synthetic_data"
train_df=create_timeseries1d(expression = '(20*np.sin(2*np.pi *2* index/100))',start_date = '1/1/2000')
train_df.to_csv(data_folder+'/train_complex_sin_timeseries.csv')
test_df=create_timeseries1d(expression = '(20*np.sin(2*np.pi *2* index/100))',start_date = '1/1/2002',periods=60)
test_df.to_csv(data_folder+'/test_complex_sin_timeseries.csv')

#Load data into gluonts
from gluonts.dataset.common import ListDataset,load_datasets

gluonts_ds=ListDataset( [
          {
      "start": "1/1/2000 00:00:00",
      "target": list(target),
      "feat_static_cat": []
  }
          for cat, target in enumerate([train_df["value"].values])
      ],freq="D")
test_ds=ListDataset( [
          {
      "start": "1/1/2000 00:00:00",
      "target": list(target),
      "feat_static_cat": []
  }
          for cat, target in enumerate([test_df["value"].values])
      ],freq="D")

from gluonts.model.deepar import DeepAREstimator
from gluonts.model.deepstate import DeepStateEstimator
from gluonts.model.deep_factor import DeepFactorEstimator
from gluonts.model.gp_forecaster import GaussianProcessEstimator
from gluonts.model.seq2seq import Seq2SeqEstimator
from gluonts.model.transformer import TransformerEstimator
from gluonts.model.simple_feedforward import  SimpleFeedForwardEstimator
from gluonts.model.wavenet import WaveNetEstimator, WaveNetSampler, WaveNet



from gluonts.trainer import Trainer
from gluonts.dataset.common import ListDataset,load_datasets
from gluonts.dataset.repository.datasets import get_dataset as get_dataset_gluon

from gluonts.dataset.field_names import FieldName
from gluonts.dataset.util import to_pandas
from gluonts.evaluation import Evaluator
from gluonts.evaluation.backtest import make_evaluation_predictions
from gluonts.model.predictor import Predictor
from gluonts.distribution.neg_binomial import NegativeBinomialOutput

estimator = DeepAREstimator(
    prediction_length     = 14,
    freq                  = "D",
    context_length          =1,
    use_feat_dynamic_real = False,
    use_feat_static_cat   = False,
    cardinality           = None,
    scaling=False,
    trainer               = Trainer(
    learning_rate         = 5e-3,
    epochs                = 10,
    num_batches_per_epoch = 10,
    batch_size            = 30
    
    )

)

predictor = estimator.train(gluonts_ds)



from gluonts.evaluation.backtest import make_evaluation_predictions 
import json


for i in range(0,3):
  test_ds=ListDataset( [
          {
      "start":str(test_df["date"].values[i]),
      "target": list(target),
      "feat_static_cat": []
  }
          for cat, target in enumerate([test_df["value"].values[i:i+25]])
      ],freq="D")

  forecast_it, ts_it = make_evaluation_predictions(
    dataset=test_ds,
    predictor=predictor,
    num_samples=10
  )

  forecasts = list(forecast_it)
  tss = list(ts_it)
  forecast_entry = forecasts[0]
  print(f"Number of sample paths: {forecast_entry.num_samples}")
  print(f"Dimension of samples: {forecast_entry.samples.shape}")
  print(f"Start date of the forecast window: {forecast_entry.start_date}")
  print(f"Frequency of the time series: {forecast_entry.freq}")
  ts_entry = tss[0]
  plot_prob_forecasts(ts_entry, forecast_entry)
  evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])
  agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(test_ds))
  print(json.dumps(agg_metrics, indent=4))

### dataset loaded once wil be deleted
dataset_path  =Path(gluonts_datafolder)

TD             =load_datasets( 
                    metadata=dataset_path,
                train=dataset_path / "train",
                test=dataset_path / "test",)

"""

###########################################
SAMPLE JSON 
##########################################
"""
true=True
false=False
jpars={
    "deepar": {
         "model_pars": {
             "model_uri"  : "model_gluon.gluonts_model",
             "model_name" : "deepar",
             "model_pars" : {
                 "prediction_length": 14, 
                 "freq": "D",
                 "distr_output" :  "StudnetOutput", 
              

                
                 "use_feat_static_cat": false, 
                 "use_feat_static_real":false,
                 
                  "num_layers": 2, 
                  "num_cells": 40, 
                  "cell_type": "lstm", 
                  "dropout_rate": 0.1,

                 
                  "scaling": false, 
                  "num_parallel_samples": 100,
                  "cardinality":  "None"
             }
             
             
             },
        "data_pars": {
            "train": true, 
            "dt_source": "",
           "data_type":"single_dataframe",

                
                 "use_feat_static_cat": false, 
                 "use_feat_static_real":false,

           
            "submission": false , 
            "dataset_name": "" , 
            
            "train_data_path":  "kaggle_data/synthetic_data/train_complex_sin_timeseries.csv" , 
             "test_data_path":  "kaggle_data/synthetic_data/test_complex_sin_timeseries.csv" , 
            "single_pred_length":28,
            "freq": "1D",
            "start" : "2001/1/1 00:00:00",        
            "startdate" : "2001/1/1",  
            "col_date"   : "date",                
            "col_ytarget" : ["value"],
            "num_series" : 1,

            "cols_cat": [],   "cols_num" : []
                    
            },
            
            
        "compute_pars": {
            "num_samples": 100,
            
            "learning_rate"         : 5e-3,
            "epochs"                : 100,
            "num_batches_per_epoch" : 20,
            "batch_size"            : 32,   
            
            "compute_pars" : {
                
                "batch_size": 32, "clip_gradient": 100, "epochs": 20, "init": "xavier", "learning_rate": 5e-3, 
                "learning_rate_decay_factor": 0.5, 
                "hybridize": false,
               
                "num_batches_per_epoch": 10,
                
                "minimum_learning_rate": 5e-05, "patience": 10, "weight_decay": 1e-08
            }
        },
        
      "out_pars": {
         "path": "ztest/model_gluon/gluonts_deepar/",
         "plot_prob": true, "quantiles": [0.5]
      }
    }
}

"""

https://github.com/arita37/mlmodels/blob/dev/mlmodels/model_gluon/gluonts_model.py



"""
# import library
import mlmodels
from mlmodels.models import module_load
from mlmodels.util import path_norm_dict, path_norm, params_json_load
import json


#json_path="m4_daily_gluonts.json"
#### Model URI and Config JSON
#model_uri   = "model_keras.gluonts_model"
#config_path = path_norm( 'model_keras/ardmn.json'  )
#config_mode = "test"  ### test/prod
model_uri="gluonts_model"
#module=None


#### Model Parameters
#jpars=None
#jpars = json.load(open(json_path))
#print(jpars)

for x in [ 'model_pars', 'data_pars', 'compute_pars', 'out_pars' ] :
  globals()[x] = jpars['deepar'][x]
print( model_pars, data_pars, compute_pars, out_pars)

#### Setup Model 
module         = module_load( model_uri)
#model          = module.Model(model_pars, data_pars, compute_pars) 

#import gluonts_model as module


model=module.Model(model_pars=model_pars, data_pars=data_pars, compute_pars=compute_pars)

#### Fit          >>>>USE module.fit

model = module.fit(model=model,data_pars= data_pars,compute_pars= compute_pars, out_pars=out_pars)           #### fit model
y_pred=module.predict(model=model,data_pars= data_pars,compute_pars= compute_pars, out_pars=out_pars) 
 
metrics_val    = module.fit_metrics(y_pred, data_pars, compute_pars, out_pars)   #### Check fit metrics
print(metrics_val)


module.plot_prob_forecasts(y_pred)
